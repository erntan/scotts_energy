---
title: "Scottsdale Energy Consumption 2020"
output: html_notebook
---
# Importing and Cleaning the Data
We first import the data as `raw`, format the column names, and format column data types.
```{r}
raw <- read.csv("https://www.phoenixopendata.com/
        dataset/598be280-a5ce-4c77-b802-321756332dfb/
        resource/d49c88c7-93b4-4c83-9032-31e4fd620f61/
        download/20210519_opendata-1.csv")

formatted <- raw

# Format the column names.
colnames(formatted) <- tolower(gsub(".", "_", colnames(raw), fixed = TRUE))

# Format the columns.
d$building_type <- tolower(d$building_type)

formatted$latitude <- as.numeric(formatted$latitude)
formatted$longitude <- as.numeric(formatted$longitude)

# Drop unneeded data.
d$year <- NULL # The year column, as all the data is from a single year (2020).
d$electric_utility <- NULL
```
## `NA` Values
Let us observe which columns contain `NA` values and handle them appropriately. We will do this by checking the unique values for each of the columns of `d`.

First, we will handle the `latitude` and `longitude` columns, as `NA`s were introduced by coercion.
```{r}
subset(d, is.na(longitude) | is.na(latitude))
```
We see that we have two categories of data that contain missing values in their latitude or longitude:
1. Sites that have multiple locations, like "Transit Stops" and "Traffic Signals".
2. Sites in the Goodyear Airport (i.e. have the address 1658 S Litchfield Rd.).

In addition, we have the PHX SkyTrain Switch Yard, which contains an address but no coordinates.

We will throw out the sites that have multiple locations-- these are denoted with `building_type = "N/A"`.
```{r}
d <- d[d$building_type != "N/A" & d$building_type != " N/A ", ]
```
We now have 7 sites without complete coordinates. All 7 of these sites do have addresses.

We have records with `energy_use_intensity == NA`, but for our purposes, we will not need this data for the time being, so we will ignore it.

The rest of our values are also clear of ambiguity. Let us move on to the analysis.

# Analysis
Let us order our data by energy usage.
```{r}
d <- d[order(d$electricity_usage, decreasing = TRUE), ]

d[10:20, ]
```

```{r}
print(range(d$electricity_usage))
print(median(d$electricity_usage))
```
```{r}
barplot(d$electricity_usage,
        names.arg = d$site_name,
        las = 2)
# Need to map colors to building types.
```
We can see that there are many sites that use very little energy, while a few sites use a lot of energy.
```{r}
tail(d, 20)
```
```{r}
five_num <- fivenum(d$electricity_usage)
iqr <- five_num[4] - five_num[2]
```
```{r}
# Non-outliers
d_mid <- d[
        d$electricity_usage <= five_num[3] + 1.5 * iqr &
        d$electricity_usage >= five_num[3] - 1.5 * iqr,
        ]
```
```{r}
barplot(d_mid$electricity_usage, names.arg = d_mid$site_name)
```
```{r}
plot(d$electricity_usage, d$building_area)
```

```{r}
d[c("site_name")]

# Want to (1) remove unneeded variables
# (do natural gas, solar, etc. usage add up to electricity_usage?)
# Relationship between building type and department?
# (Now that Im typing, obviously the answer is yes.)
```

